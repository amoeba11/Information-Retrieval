# -*- coding: utf-8 -*-
"""IR-2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HsUsIZQX713UXC--cxzmsuqah1AnBblR

---
# Dolly Khandelwal 2017A7PS0952G

---

# Import Libraries and Load Data
"""

!pip install nltk
!pip install matplotlib
!pip install numpy
!pip install scipy

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from collections import defaultdict, Counter
import time
from datetime import datetime

import nltk
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.stem.snowball import PorterStemmer, SnowballStemmer
from nltk.stem.lancaster import LancasterStemmer
from sklearn.cluster import KMeans

pd.set_option('max_colwidth', None) 

CRTPOSTAGS = 'NN NNS NNP NNPS'.split()

LEM = WordNetLemmatizer()
PS = PorterStemmer()
SS = SnowballStemmer('english')
LS = LancasterStemmer()

class DataSet(object):
    def __init__(self):
      dirpath = './'
      allDocs = {}
      with open(dirpath+'documents.txt') as f:
        rawDocs = f.read().strip('/').split('/')
        for doc in rawDocs:
          doc = doc.strip().split('\n')
          allDocs[doc[0]] = ' '.join(doc[1:])

      self.docs = {}
      self.relevantDocIds = []
      with open(dirpath+'relevance_assessment.txt') as f:
        q2D = f.read().strip().strip('/').split('/')
        for q in q2D:
          q = q.strip().split('\n')
          self.relevantDocIds.append(q[1].split())
          for doc in q[1].split():
            if doc not in self.docs:
              self.docs[doc] = allDocs[doc]
      
      self.queries = []
      with open(dirpath+'query.txt') as f:
        rawQueries = f.read().strip().strip('/').split('/')
        for q in rawQueries:
          q = q.strip().split('\n')
          self.queries.append(q[1].strip())
      
      self.editedQueries = ['INORMAION ON HIGH CUREENT TRANTISSOR SWITCHES',
                            'INFORMATION ON DESGN OF TIJE DICISIN MULTPLEING CIRCUUTS',
                            'DLTAIES OF AAILABOE LOW VLTAE CAPCITRS',
                            'DESIGN OF DIRECT COIPLED FLI FLOPS TO FUNCTION WHTI THE MMXIAUM VQRIATIONS IN THE VAKUES OF THE CIRDUIT COMPONENTS',
                            'PLEASE SUPPLY INFARMOTION ON TE PEEFORMANCE OF TYPICL MAFNETIC FILM MENORY SYSTEMS WITH CIRCUIT DIAGRAMS',
                            'I WLUOD LIKE DSTAILE OF THE WPRK WHICH HAS BEN DONE TO EZTEND THE FREQIENCY RENGA OF MAGNEIC AMPIFIERS',
                            'I WOLD LIKE INFOMAION ON THE RENGA OF SRATIC RELSYS SUITABLE FOR USE AT HIGH SQITCHING RATES',
                            'I AM ITTERESNED IN CIRCYOTRY CAPABLE OF GENEEATING EXTTEMELY NARROW PULSES',
                            'PLEASE SUPLY INFORMATION ON THE TJEORY AND USE OF PRAMETRIC AMPKUFIERS',
                            'THE SSNTHESIY OF NTWORS WITH GICEN SSMLLWD DATA TRABSFER FUNCIONS']
    
    def calcMetrics(self, relevant, returned):
      relevantReturned = 0
      for r in returned:
        if r[0] in relevant:
          relevantReturned += 1
      return relevantReturned/len(returned) , relevantReturned/len(relevant), len(returned), relevantReturned, len(relevant)
    
    def plotMetrics(self, precision, recall, x=None, title='Query Number', leg1='Precision', leg2='Recall'):
      x = np.arange(1, len(precision)+1)
      ax1 = plt.subplot(1,1,1)
      w = 0.3
      plt.xticks(x + w /2, x)
      prec = ax1.bar(x, precision, width=w, color='b', align='center')
      rec = ax1.bar(x + w, recall, width=w, color='g',align='center')
      plt.xlabel(title)
      plt.legend([prec, rec],[leg1, leg2])
      plt.show()

    def plotMetrics2(self, bars=[], title='Title', leg=[]):
      barWidth = 0.2
      # Set position of bar on X axis
      r1 = np.arange(len(bars[0]))
      r2 = [x + barWidth for x in r1]
      r3 = [x + barWidth for x in r2]
      r4 = [x + barWidth for x in r3]

      # Make the plot
      plt.bar(r1, bars[0], color='b', width=barWidth, edgecolor='white', label=leg[0])
      plt.bar(r2, bars[1], color='g', width=barWidth, edgecolor='white', label=leg[1])
      plt.bar(r3, bars[2], color='r', width=barWidth, edgecolor='white', label=leg[2])
      plt.bar(r4, bars[3], width=barWidth, edgecolor='white', label=leg[3])

      # Add xticks on the middle of the group bars
      plt.xlabel('group', fontweight='bold')
      x = np.arange(1,11)
      plt.xticks([r + barWidth for r in range(len(bars[0]))], x)

      # Create legend & Show graphic
      plt.legend()
      plt.show()

    def executeQueries(self, invInd, K=20, useEditedQueries=False, disp=True, retTimeTaken=False):
      if useEditedQueries:
        queries = self.editedQueries
      else:
        queries = self.queries
      p = []
      r = []
      ret = []
      topDocs = []
      queryID  = []
      docIDs = []
      scores = []
      timeTaken = []

      idx = 0
      for query, relevant in zip(queries, self.relevantDocIds):
        now = time.time()
        topDocsReturned = invInd.queryTopKDocs(query, K)
        # print("Docs returned for query: ",query)
        # print(topDocsReturned)
        precision, recall, returned, relevantReturned, relevant = self.calcMetrics(relevant, topDocsReturned)
        p.append(precision)
        r.append(recall)
        ret.append(returned)
        topDocs.append(topDocsReturned)

        queryID.append(idx+1)
        docIDs.append([x[0] for x in topDocsReturned])
        scores.append([round(x[1],2) for x in topDocsReturned])
        timeTaken.append(time.time() - now)

        # if disp:
          # print("Relevant : ", relevant, ", Returned : ", returned, ", RelevantReturned : ", relevantReturned, ", Precision : ", precision, ", Recall : ", recall)
          # print('')
        
        idx += 1

      if disp:
        self.plotMetrics(p, r)
        print("Avg Precision : ", sum(p)/10, ", Recall : ", sum(r)/10, ", Avg Returned : ", sum(ret)/10)


      df = pd.DataFrame()
      df["QueryID"] = queryID
      df["Doc IDs"] = docIDs
      df["Scores"] = scores
      
      if retTimeTaken:
        return p, r, ret, df, timeTaken
      else:
        return p, r, ret, df
    
    def checkQueryHealth(self, invInd, queries=None, editDistance=4):
      if queries is None:
        queries = self.editedQueries
      queryIDs, queryTypos, fixedTerms, typoDistances = [], [], [], []

      for idx, query in enumerate(queries):
        qT, fT, tD = invInd.processTermMismatches(query.lower().split(), editDistance)[1:]
        queryIDs += [idx+1]*len(qT)
        queryTypos += qT
        fixedTerms += fT
        typoDistances += tD
        # print(qT)
      
      df = pd.DataFrame()
      df["Query ID"] = queryIDs
      df["Typo"] = queryTypos
      df["Term"] = fixedTerms
      df["Distance"] = typoDistances
      pd.set_option("display.max_rows", None, "display.max_columns", None)
      return df
    
data = DataSet()

"""# Index Model"""

class IndexModel(object):
 
  def __init__(self, docs, processTypes=[], freqThres=200, editDistThres=4, numClusters=5):
    self.numDocs = len(docs)
    self.invIndList = dict()   #dict maps terms to doc ids
    self.termFreq_tf = dict()  #dict maps doc ids to terms which are mapped to term freqs -- tf td
    self.docFreq_df = dict()   #dict maps terms to num doc freq
    self.actualTerms = set()   #set of actual terms in corpus
    self.termIndex = {}        #dict maps terms to term index 
    self.docIndex = {}         #dict maps docIds to docId index

    # LEM, STEM, PREPO, SUM, COSINE, LNDIST, QWDIST, CLUSTER
    self.processTypes = processTypes
    self.freqThres = np.log10(self.numDocs/freqThres)
    self.editDistThres = editDistThres
    self.numClusters = numClusters

    #Construct Inverted Index, tf & df
    for docId, doc in docs.items():
      self.termFreq_tf[docId] = {}
      self.docIndex[docId] = len(self.docIndex)
      for term in doc.split():
        self.actualTerms.add(term)
        term = self.processTerm(term)
        if term not in self.invIndList:
          self.invIndList[term] = set()
          self.docFreq_df[term] = 0
          self.termIndex[term] = len(self.termIndex)
        if docId not in self.invIndList[term]:
          self.invIndList[term].add(docId)
          self.termFreq_tf[docId][term] = 0
          self.docFreq_df[term] += 1
        self.termFreq_tf[docId][term] += 1
    
    #Construct weighted tf and df values
    self.tfDval = {}
    self.tfType = 'AUG'
    self.construct_tf()
    self.dfType = 'IDF'
    self.construct_df()
    
    #Make Vector for docs
    self.docTfidfVector = np.zeros((self.numDocs, len(self.docFreq_df.keys())))
    for docId, docIdx in self.docIndex.items():
      for term, termIdx in self.termIndex.items():
        self.docTfidfVector[docIdx][termIdx] = self.get_tf(term, docId)#*self.get_idf(term)
    
    if 'CLUSTER' in self.processTypes:
      self.clusteringModel = KMeans(n_clusters=self.numClusters)
      self.clusteringModel.fit(self.docTfidfVector)

  
  def construct_tf(self):
    if self.tfType == 'L':
      for docId in self.termFreq_tf:
        for term in self.termFreq_tf[docId]:
          self.termFreq_tf[docId][term] = 1 + np.log10(self.termFreq_tf[docId][term])
    
    elif self.tfType == 'L_AVG':
      for docId in self.termFreq_tf:
        self.tfDval[docId] = 1 + np.log10(np.average(list(self.termFreq_tf[docId].values())))
        for term in self.termFreq_tf[docId]:
          self.termFreq_tf[docId][term] = 1 + np.log10(self.termFreq_tf[docId][term])
    
    elif self.tfType == 'AUG':
      for docId in self.termFreq_tf:
        self.tfDval[docId] = 2 * np.max(list(self.termFreq_tf[docId].values()))
    
  def construct_df(self):
    if self.dfType == 'IDF':
      for term in self.docFreq_df:
        self.docFreq_df[term] = np.log10(self.numDocs/self.docFreq_df[term])
    
    elif self.dfType == 'P_IDF':
      for term in self.docFreq_df:
        try:
          self.docFreq_df[term] = max(0, np.log10(self.numDocs/self.docFreq_df[term] - 1))
        except:
          self.docFreq_df[term] = 0
  
  def removeStopWords(self, terms):
    retTerms = []
    for term in terms:
      if term in self.docFreq_df and self.docFreq_df[term] >= self.freqThres:   #greater because thres is normalized
        retTerms.append(term)
    return retTerms
  
  def processTerm(self, term):
    term = term.lower()
    if 'LEM' in self.processTypes:
      return LEM.lemmatize(term)
    elif 'STEM' in self.processTypes:
      return PS.stem(term)
    else:
      return term
  
  def processQueryTerms(self, query):
    terms = query.lower().split()
    
    if 'LNDIST' in self.processTypes or 'QWDIST' in self.processTypes:
      terms = self.processTermMismatches(terms, self.editDistThres)[0]
    
    if 'PREPO' in self.processTypes:
      queryPosTags = nltk.pos_tag(terms)
      try:
        ind = queryPosTags.index('IN')
        terms = terms[:ind+1]
      except:
        pass
    
    retTerms = [self.processTerm(term) for term in terms]
    return retTerms

  def processTermMismatches(self, terms, editDistance=4):
    queryTypos = []
    fixedTerms = []
    typoDistances = []
    fixedQueryTerms = []
    ED = ErrorDistance()
    
    for queryTerm in terms:
      if queryTerm in self.actualTerms:
        fixedQueryTerms.append(queryTerm)
        continue
      # print(queryTerm)
      minDist = editDistance
      minDistTerms = []
      for actualTerm in self.actualTerms:
        if 'LNDIST' in self.processTypes:
          currDist = ED.levenshteinDistance(queryTerm, actualTerm)
        elif 'QWDIST' in self.processTypes:
          currDist = ED.qwertyDistance(queryTerm, actualTerm)
        if currDist == minDist:
          minDistTerms.append(actualTerm)
        elif currDist < minDist:
          minDist = currDist
          minDistTerms = [actualTerm]
      
      if len(minDistTerms) > 0:
        fixedQueryTerms.append(minDistTerms[0])
        queryTypos.append(queryTerm)
        fixedTerms.append(minDistTerms)
        typoDistances.append(minDist)
      else:
        fixedQueryTerms.append(queryTerm)
    
    return fixedQueryTerms, queryTypos, fixedTerms, typoDistances
  
  def get_tf(self, term, docId):
    if docId in self.termFreq_tf and term in self.termFreq_tf[docId]:
      if self.tfType == 'L':
        return self.termFreq_tf[docId][term]
      elif self.tfType == 'L_AVG':
        return self.termFreq_tf[docId][term]/(self.tfDval[docId])
      elif self.tfType == 'AUG':
        return 0.5 + (self.termFreq_tf[docId][term]/self.tfDval[docId])
    else:
      return 0
  
  def get_idf(self, term):
    if term in self.docFreq_df:
      return self.docFreq_df[term]
    else:
      return 0
  
  def get_tfidf(self, term, docId):
    return self.get_tf(term, docId)*self.get_idf(term)

  def get_query_tfidf(self, term, queryTermFreq):
    return (1 + np.log10(queryTermFreq[term]))*self.get_idf(term)
     
  def queryDocScore_Sum(self, queryTerms, docId):
    docTerms = self.termFreq_tf[docId].keys()
    terms = set(self.removeStopWords(set(docTerms) & set(queryTerms)))
    # terms = set(docTerms) & set(queryTerms)
    score = 0
    for term in terms:
      score += self.get_tfidf(term, docId)
    return score
  
  def queryDocScore_Cosine(self, queryTerms, queryTfIdf, docId):
    score = 1 -  np.inner(queryTfIdf, self.docTfidfVector[self.docIndex[docId]])/(np.linalg.norm(self.docTfidfVector[self.docIndex[docId]]))
    if np.isnan(score):
      score = 100000

    return score
  
  def queryTopKDocs(self, query, K=20):
    scores = []
    queryTerms = self.processQueryTerms(query)
    # print(self.removeStopWords(queryTerms))

    if 'COSINE' in self.processTypes:
      queryTfIdf = np.zeros(len(self.docFreq_df.keys()))
      queryTf = Counter(queryTerms)

      if self.tfType == 'L':
        tfD = 1
      elif self.tfType == 'L_AVG':
        tfD = 1 + np.log10(sum(queryTf.values())/len(queryTf))
      elif self.tfType == 'AUG':
        tfD = max(queryTf.values())
      
      terms = self.removeStopWords(set(queryTerms))
      for term, freq in queryTf.items():
        if term in terms:
          if self.tfType == 'L' or self.tfType =='L_AVG':
            queryTfIdf[self.termIndex[term]] = (1 + np.log10(freq))*self.get_idf(term)/tfD
          elif self.tfType == 'AUG':
            queryTfIdf[self.termIndex[term]] = (0.5 + 0.5*freq/tfD)*self.get_idf(term)
      queryTfIdf2D = np.reshape(queryTfIdf, (1, len(queryTfIdf)))
      
      if 'CLUSTER' in self.processTypes:
        cluster = self.clusteringModel.predict(queryTfIdf2D)[0]

    for docId in self.termFreq_tf:
      if 'CLUSTER' not in self.processTypes or self.clusteringModel.labels_[self.docIndex[docId]] == cluster:
        score = 0
        if 'SUM' in self.processTypes:
          score = self.queryDocScore_Sum(queryTerms, docId)
        elif 'COSINE' in self.processTypes:
          score = self.queryDocScore_Cosine(queryTerms, queryTfIdf, docId)
        scores.append([docId, score])
    
    if 'SUM' in self.processTypes:
      scores.sort(key=lambda x: x[1], reverse=True)
    elif 'COSINE' in self.processTypes:
      scores.sort(key=lambda x: x[1])
    topDocs = scores[:K]
    return topDocs
  
  def printAllVars(self):
    print("InvList:", self.invIndList)
    print("tf:", self.termFreq_tf)
    print("df:", self.docFreq_df)

"""# Task 1

a) Mfull - Stopword Removal and score with weighted tf-idf
"""

Mfull = IndexModel(data.docs, processTypes={'SUM'}, freqThres=40)

P, R, Ret, TopDocs = data.executeQueries(Mfull, K=20, disp=False)
TopDocs

"""b) Mp - Stopword Removal, Stemming & weighted tf-idf in lnc.ltc form"""

MP = IndexModel(data.docs, processTypes={'STEM','COSINE'}, freqThres=60)

print('Top 2 Precision and Recall:')
MP_P2, MP_R2, MP_Ret2, MP_TopDocs2 = data.executeQueries(MP, K=2, disp=True)
print('')
print('Top 5 Precision and Recall:')
MP_P5, MP_R5, MP_Ret5, MP_TopDocs5 = data.executeQueries(MP, K=5, disp=True)
print('')
print('Top 10 Precision and Recall:')
MP_P10, MP_R10, MP_Ret10, MP_TopDocs10 = data.executeQueries(MP, K=10, disp=True)



"""**c)** Firstly, Mfull model was made in which words with collection frequency more than 60 were declared as stopwords and removed. The same threshold is followed for all the models in this notebook. Then weighted tf-idf scores were calculated and used to rank the documents. For tf, augmented form was used and for idf normal log weighting was used. The same format for calculating tf-idf scores is followed throughout whenever needed. In the cell below you can see the results in detail for Mfull model. 

Next we implement stemming along with weighted tf-idf. The weighting scheme used is lnc.ltc - document: logarithmic tf, no idf and cosine normalization; query: logarithmic tf, idf, no normalization. Finally we use cosine distance for ranking. This comprises our Mp model.

Precision comparison overview table:

    Metric              |  Mfull   |  Mp   |    
    Top 2 Precision     |  0.75    | 0.9   |   
    Top 5 Precision     |  0.66    | 0.74  |  
    Top 10 Precision    |  0.5     | 0.64  |

Results with Mfull model:
"""

print('Top 2 Precision and Recall:')
Mf_P2, Mf_R2, Mf_Ret2, Mf_TopDocs2 = data.executeQueries(Mfull, K=2, disp=True)
print('')
print('Top 5 Precision and Recall:')
Mf_P5, Mf_R5, Mf_Ret5, Mf_TopDocs5 = data.executeQueries(Mfull, K=5, disp=True)
print('')
print('Top 10 Precision and Recall:')
Mf_P10, Mf_R10, Mf_Ret10, Mf_TopDocs10 = data.executeQueries(Mfull, K=10, disp=True)



"""# Task 2"""

class KeyBoardLayout(object):
  def __init__(self, layout=None):
    if layout is None:
      # self.layout = ['q w e r t y u i o p',' a s d f g h j k l','  z x c v b n m']
      self.layout = ['qwertyuiop','asdfghjkl','zxcvbnm']
    else:
      self.layout = layout
    self.buildAdjKeysList()
  
  def buildAdjKeysList(self):
    self.adjKeys = {}
    pos = {}

    for i in range(len(self.layout)):
      for j in range(len(self.layout[i])):
        pos[(i,j)] = self.layout[i][j]

    def addIfKeyExists(self, i, j, l):
      if (i,j) in pos:
        self.adjKeys[l].add(pos[(i,j)])

    for ij in pos:
      l = pos[ij]
      self.adjKeys[l] = set()
      addIfKeyExists(self,ij[0]-1,ij[1]-1,l)
      addIfKeyExists(self,ij[0]-1,ij[1],l)
      addIfKeyExists(self,ij[0]-1,ij[1]+1,l)
      addIfKeyExists(self,ij[0],ij[1]-1,l)
      addIfKeyExists(self,ij[0],ij[1]+1,l)
      addIfKeyExists(self,ij[0]+1,ij[1]-1,l)
      addIfKeyExists(self,ij[0]+1,ij[1],l)
      addIfKeyExists(self,ij[0]+1,ij[1]+1,l)

    # print(self.adjKeys)

class ErrorDistance(object):
  def levenshteinDistance(self, w1, w2, cost={'a':1,'d':1,'s':2}):
    distanceMatrix = [[0 for x in range(len(w2)+1)] for x in range(len(w1)+1)]
    
    for i in range(len(w1)+1): 
      for j in range(len(w2)+1): 
        if i == 0: 
          distanceMatrix[i][j] = j*cost['a']
        elif j == 0: 
          distanceMatrix[i][j] = i*cost['d']
        elif w1[i-1] == w2[j-1]: 
          distanceMatrix[i][j] = distanceMatrix[i-1][j-1]
        else: 
          distanceMatrix[i][j] = min(distanceMatrix[i][j-1] + cost['a'],        # Insert 
                                     distanceMatrix[i-1][j] + cost['d'],        # Delete
                                     distanceMatrix[i-1][j-1] + cost['s'])      # Substitution 
    
    return distanceMatrix[len(w1)][len(w2)]

  def qwertyDistance(self, w1, w2, cost={'a':1,'d':1,'s':2}, layout=None):
    if w1[0] != w2[0]:
      return 10000
    
    KBL = KeyBoardLayout(layout)
    distanceMatrix = [[0 for x in range(len(w2)+1)] for x in range(len(w1)+1)]

    for i in range(len(w1)+1): 
      for j in range(len(w2)+1): 
        if i == 0: 
          distanceMatrix[i][j] = j*cost['a']
        elif j == 0: 
          distanceMatrix[i][j] = i*cost['d']
        elif w1[i-1] == w2[j-1]:
          distanceMatrix[i][j] = distanceMatrix[i-1][j-1]
        else:
          if w1[i-1] in KBL.adjKeys[w2[j-1]]:
            subCost = cost['s']/2
          else:
            subCost = cost['s'] 
          distanceMatrix[i][j] = min(distanceMatrix[i][j-1] + cost['a'],        # Insert 
                                      distanceMatrix[i-1][j] + cost['d'],        # Delete
                                      distanceMatrix[i-1][j-1] + subCost)      # Substitution 

    return distanceMatrix[len(w1)][len(w2)]

"""a) Levenshtein edit distance with threshold=3

"""

Mspell = IndexModel(data.docs, processTypes={'STEM','COSINE','LNDIST'}, freqThres=60, editDistThres=3)

data.checkQueryHealth(Mspell, data.editedQueries, editDistance=3)

"""b) Mspell model is Mp model with Levenshtein edit distance for query term error correction.

Mnospell model is Mp model without Levenshtein edit distance for query term error correction.
"""

Mspell = IndexModel(data.docs, processTypes={'STEM','COSINE','LNDIST'}, freqThres=60, editDistThres=3)
Mnospell = IndexModel(data.docs, processTypes={'STEM','COSINE'}, freqThres=60)

"""Top 5 Precision Recall:"""

print('Top 5 Precision and Recall without Levenshtien correction:')
MSp_P5, MSp_R5, MSp_Ret2, MSp_TopDocs2 = data.executeQueries(Mnospell, K=5, useEditedQueries=True, disp=True)
print('')
print('Top 5 Precision and Recall with Levenshtien correction :')
MSp_P5, MSp_R5, MSp_Ret2, MSp_TopDocs2 = data.executeQueries(Mspell, K=5, useEditedQueries=True, disp=True)

"""Top 10 Precision Recall:"""

print('Top 10 Precision and Recall without Levenshtien correction:')
MSp_P10, MSp_R10, MSp_Ret10, MSp_TopDocs10 = data.executeQueries(Mnospell, K=10, useEditedQueries=True, disp=True)
print('')
print('Top 10 Precision and Recall with Levenshtien correction :')
MSp_P10, MSp_R10, MSp_Ret10, MSp_TopDocs10 = data.executeQueries(Mspell, K=10, useEditedQueries=True, disp=True)



"""c) QWERTY edit distance with threshold=3

"""

Mqwerty = IndexModel(data.docs, processTypes={'STEM','COSINE','QWDIST'}, freqThres=60, editDistThres=3)

data.checkQueryHealth(Mqwerty, data.editedQueries, editDistance=3)

"""d) Mqwerty model is Mp model with QWERTY edit distance for query term error correction.

Mnoqwerty model is Mp model without QWERTY edit distance for query term error correction.
"""

Mqwerty = IndexModel(data.docs, processTypes={'STEM','COSINE','QWDIST'}, freqThres=60, editDistThres=3)
Mnoqwerty = IndexModel(data.docs, processTypes={'STEM','COSINE'}, freqThres=60)

print('Top 5 Precision and Recall without Qwerty correction:')
MQw_P5, MQw_R5, MQw_Ret5, MQw_TopDocs5 = data.executeQueries(Mnoqwerty, K=5, useEditedQueries=True, disp=True)
print('')
print('Top 5 Precision and Recall with Qwerty correction :')
MQw_P5, MQw_R5, MQw_Ret5, MQw_TopDocs5 = data.executeQueries(Mqwerty, K=5, useEditedQueries=True, disp=True)

print('Top 10 Precision and Recall without Qwerty correction:')
MQw_P10, MQw_R10, MQw_Ret10, MQw_TopDocs10 = data.executeQueries(Mnoqwerty, K=10, useEditedQueries=True, disp=True)
print('')
print('Top 10 Precision and Recall with Qwerty correction :')
MQw_P10, MQw_R10, MQw_Ret10, MQw_TopDocs10 = data.executeQueries(Mqwerty, K=10, useEditedQueries=True, disp=True)

"""e) Qwerty edit distance assigns less weightage to natural errors that could be produced by the Qwerty ordering of the keyboard. Keys that  are nearby could easily be mispressed thus leading to an error. Employing this knowledge we can see that the Qwerty edit distance correction offers more robust results on the queries presented, as along with accounting for the deletion, addition and substitution of characters in input query terms also accounts for the natural errors produced by keyboard placement with lesser weightage.

Therefore error terms like ampkufiers in Query 9 are converted to [amplifiers] with a levenshtein edit distance of 4 whereas the qwerty distance between them is 2. Similarly for ssmllwd in Query 10 is converted to sampled by the Qwerty algorithm well within the threshold of 3 whereas is not taken into account by Levenshtein Distance itself

"""



"""# Task 3

a) Time Taken by each model
"""

Mfull = IndexModel(data.docs, processTypes={'SUM'}, freqThres=40)
MP = IndexModel(data.docs, processTypes={'STEM','COSINE'}, freqThres=60)
Mspell = IndexModel(data.docs, processTypes={'STEM','COSINE','LNDIST'}, freqThres=60, editDistThres=3)
Mqwerty = IndexModel(data.docs, processTypes={'STEM','COSINE','QWDIST'}, freqThres=60, editDistThres=3)

#Mfull
MF_P, MF_R, MF_Ret, MF_TopDocs, MF_TimeTaken = data.executeQueries(Mfull, K=10, retTimeTaken=True, disp=False)
print("Average Secs:",sum(MF_TimeTaken)/10)

#MP
MP_P10, MP_R10, MP_Ret10, MP_TopDocs10, MP_TimeTaken = data.executeQueries(MP, K=10, retTimeTaken=True, disp=False)
print("Average Secs:",sum(MP_TimeTaken)/10)

#Mspell
MSp_P10, MSp_R10, MSp_Ret10, MSp_TopDocs10, MSp_TimeTaken = data.executeQueries(Mspell, K=10, useEditedQueries=True, retTimeTaken=True, disp=False)
print("Average Secs:", sum(MSp_TimeTaken)/10)

#Mqwerty
MQw_P10, MQw_R10, MQw_Ret10, MQw_TopDocs10, MQw_TimeTaken = data.executeQueries(Mqwerty, K=10, useEditedQueries=True, retTimeTaken=True, disp=False)
print("Average Sec:", sum(MQw_TimeTaken)/10)

timeTaken = [MF_TimeTaken, MP_TimeTaken, MSp_TimeTaken, MQw_TimeTaken]
models = ['Mfull', 'Mp', 'Mspell', 'Mqwerty']
data.plotMetrics2(timeTaken, 'Time Taken', models)



Mfull = IndexModel(data.docs, processTypes={'SUM'}, freqThres=40)
MP = IndexModel(data.docs, processTypes={'STEM','COSINE','CLUSTER'}, freqThres=60)
Mspell = IndexModel(data.docs, processTypes={'STEM','COSINE','LNDIST','CLUSTER'}, freqThres=60, editDistThres=3)
Mqwerty = IndexModel(data.docs, processTypes={'STEM','COSINE','QWDIST','CLUSTER'}, freqThres=60, editDistThres=3)

#Mfull
MF_P, MF_R, MF_Ret, MF_TopDocs, MF_TimeTaken = data.executeQueries(Mfull, K=10, retTimeTaken=True, disp=False)
print("Average Secs:",sum(MF_TimeTaken)/10)

#MP
MP_P10, MP_R10, MP_Ret10, MP_TopDocs10, MP_TimeTaken = data.executeQueries(MP, K=10, retTimeTaken=True, disp=False)
print("Average Secs:",sum(MP_TimeTaken)/10)

#Mspell
MSp_P10, MSp_R10, MSp_Ret10, MSp_TopDocs10, MSp_TimeTaken = data.executeQueries(Mspell, K=10, useEditedQueries=True, retTimeTaken=True, disp=False)
print("Average Secs:", sum(MSp_TimeTaken)/10)

#Mqwerty
MQw_P10, MQw_R10, MQw_Ret10, MQw_TopDocs10, MQw_TimeTaken = data.executeQueries(Mqwerty, K=10, useEditedQueries=True, retTimeTaken=True, disp=False)
print("Average Sec:", sum(MQw_TimeTaken)/10)

timeTaken = [MF_TimeTaken, MP_TimeTaken, MSp_TimeTaken, MQw_TimeTaken]
models = ['Mfull', 'Mp', 'Mspell', 'Mqwerty']
data.plotMetrics2(timeTaken, 'Time Taken', models)



"""c ) As we are using the vector space model, we choose to go with clustering our vectors and retrieving the entire closest cluster as it’s computationally easier to divide vectors into subsets and leads to less number of comparisons. As K-means clustering basically depends on the distance calculation of each data point in a cluster from it’s center, given that we have a small dataset, the time cost of our model is less and thus it turns out to be more efficient.
Also, as the number of queries increase, classification in clusters is still fast and takes less number of comparisons.

We have divided the vector space into 10 clusters and it has proven to give the best results with the least reduction of precision of results. As clustering does influence the documents that are being retrieved it has to be certainly taken into account while looking at precision and recall results for a model. This heuristic cannot be applied to MFull as MFull is based on a weighted Tf-Idf Model that sums up tf-idf weights to calculate the score. For Mp, Mspell and MQwerty we can observe that the execution time reduces after implementing clustering.

If the no. of documents and queries are large we can clearly see the big advantage of using clustering to group similar documents together.
"""

